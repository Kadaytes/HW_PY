{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcwl9bwKMfwu261fMgY0zU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kadaytes/HW_PY/blob/main/DZ_7_VNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BsFTp5pmwPaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ДЗ 7.  Детектирование объектов"
      ],
      "metadata": {
        "id": "pPd8xPhlvF8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "поразбирался с кодом приведенным в материалах к уроку"
      ],
      "metadata": {
        "id": "FcHah0n0wEC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BrqFRtycI7lr"
      },
      "outputs": [],
      "source": [
        "#ssdvg\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "class DLProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.last_block = block_num\n",
        "        self.update((block_num - self.last_block) * block_size)"
      ],
      "metadata": {
        "id": "kWY3ZWvqkERK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def conv_map(x, size, shape, stride, name, padding='SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable(\"filter\",\n",
        "                            shape=[shape, shape, x.get_shape()[3], size],\n",
        "                            initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b = tf.Variable(tf.zeros(size), name='biases')\n",
        "        x = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)\n",
        "        x = tf.nn.bias_add(x, b)\n",
        "        x = tf.nn.relu(x)\n",
        "        l2 = tf.nn.l2_loss(w)\n",
        "    return x, l2"
      ],
      "metadata": {
        "id": "kAmbUfBUkOIg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def classifier(x, size, mapsize, name):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable(\"filter\",\n",
        "                            shape=[3, 3, x.get_shape()[3], size],\n",
        "                            initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b = tf.Variable(tf.zeros(size), name='biases')\n",
        "        x = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = tf.nn.bias_add(x, b)\n",
        "        x = tf.reshape(x, [-1, mapsize.w*mapsize.h, size])\n",
        "        l2 = tf.nn.l2_loss(w)\n",
        "    return x, l2"
      ],
      "metadata": {
        "id": "q_HQqEo4kVMq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def smooth_l1_loss(x):\n",
        "    square_loss   = 0.5*x**2\n",
        "    absolute_loss = tf.abs(x)\n",
        "    return tf.where(tf.less(absolute_loss, 1.), square_loss, absolute_loss-0.5)"
      ],
      "metadata": {
        "id": "rrgosys6kfCm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def array2tensor(x, name):\n",
        "    init = tf.constant_initializer(value=x, dtype=tf.float32)\n",
        "    tensor = tf.get_variable(name=name, initializer=init, shape=x.shape)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "wiUv-NqVkkKB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def l2_normalization(x, initial_scale, channels, name):\n",
        "    with tf.variable_scope(name):\n",
        "        scale = array2tensor(initial_scale*np.ones(channels), 'scale')\n",
        "        x = scale*tf.nn.l2_normalize(x, axis=-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "AnxkdrGjkqDl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSDVGG:\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __init__(self, session, preset):\n",
        "        self.preset = preset\n",
        "        self.session = session\n",
        "        self.__built = False\n",
        "        self.__build_names()\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_from_vgg(self, vgg_dir, num_classes, a_trous=True,\n",
        "                       progress_hook='tqdm'):\n",
        "        \"\"\"\n",
        "        Build the model for training based on a pre-define vgg16 model.\n",
        "        :param vgg_dir:       directory where the vgg model should be stored\n",
        "        :param num_classes:   number of classes\n",
        "        :param progress_hook: a hook to show download progress of vgg16;\n",
        "                              the value may be a callable for urlretrieve\n",
        "                              or string \"tqdm\"\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes+1\n",
        "        self.num_vars = num_classes+5\n",
        "        self.l2_loss = 0\n",
        "        self.__download_vgg(vgg_dir, progress_hook)\n",
        "        self.__load_vgg(vgg_dir)\n",
        "        if a_trous: self.__build_vgg_mods_a_trous()\n",
        "        else: self.__build_vgg_mods()\n",
        "        self.__build_ssd_layers()\n",
        "        self.__build_norms()\n",
        "        self.__select_feature_maps()\n",
        "        self.__build_classifiers()\n",
        "        self.__built = True\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_from_metagraph(self, metagraph_file, checkpoint_file):\n",
        "        \"\"\"\n",
        "        Build the model for inference from a metagraph shapshot and weights\n",
        "        checkpoint.\n",
        "        \"\"\"\n",
        "        sess = self.session\n",
        "        saver = tf.train.import_meta_graph(metagraph_file)\n",
        "        saver.restore(sess, checkpoint_file)\n",
        "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.result      = sess.graph.get_tensor_by_name('result/result:0')\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_optimizer_from_metagraph(self):\n",
        "        \"\"\"\n",
        "        Get the optimizer and the loss from metagraph\n",
        "        \"\"\"\n",
        "        sess = self.session\n",
        "        self.loss = sess.graph.get_tensor_by_name('total_loss/loss:0')\n",
        "        self.localization_loss = sess.graph.get_tensor_by_name('localization_loss/localization_loss:0')\n",
        "        self.confidence_loss = sess.graph.get_tensor_by_name('confidence_loss/confidence_loss:0')\n",
        "        self.l2_loss = sess.graph.get_tensor_by_name('total_loss/l2_loss:0')\n",
        "        self.optimizer = sess.graph.get_operation_by_name('optimizer/optimizer')\n",
        "        self.labels = sess.graph.get_tensor_by_name('labels:0')\n",
        "\n",
        "        self.losses = {\n",
        "            'total': self.loss,\n",
        "            'localization': self.localization_loss,\n",
        "            'confidence': self.confidence_loss,\n",
        "            'l2': self.l2_loss\n",
        "        }\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __download_vgg(self, vgg_dir, progress_hook):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Check if the model needs to be downloaded\n",
        "        #-----------------------------------------------------------------------\n",
        "        vgg_archive = 'vgg.zip'\n",
        "        vgg_files   = [\n",
        "            vgg_dir + '/variables/variables.data-00000-of-00001',\n",
        "            vgg_dir + '/variables/variables.index',\n",
        "            vgg_dir + '/saved_model.pb']\n",
        "\n",
        "        missing_vgg_files = [vgg_file for vgg_file in vgg_files \\\n",
        "                             if not os.path.exists(vgg_file)]\n",
        "\n",
        "        if missing_vgg_files:\n",
        "            if os.path.exists(vgg_dir):\n",
        "                shutil.rmtree(vgg_dir)\n",
        "            os.makedirs(vgg_dir)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Download vgg\n",
        "            #-------------------------------------------------------------------\n",
        "            url = 'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip'\n",
        "            if not os.path.exists(vgg_archive):\n",
        "                if callable(progress_hook):\n",
        "                    urlretrieve(url, vgg_archive, progress_hook)\n",
        "                else:\n",
        "                    with DLProgress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
        "                        urlretrieve(url, vgg_archive, pbar.hook)\n",
        "            #-------------------------------------------------------------------\n",
        "            # Extract vgg\n",
        "            #-------------------------------------------------------------------\n",
        "            zip_archive = zipfile.ZipFile(vgg_archive, 'r')\n",
        "            zip_archive.extractall(vgg_dir)\n",
        "            zip_archive.close()\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __load_vgg(self, vgg_dir):\n",
        "        sess = self.session\n",
        "        graph = tf.saved_model.loader.load(sess, ['vgg16'], vgg_dir+'/vgg')\n",
        "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.vgg_conv4_3 = sess.graph.get_tensor_by_name('conv4_3/Relu:0')\n",
        "        self.vgg_conv5_3 = sess.graph.get_tensor_by_name('conv5_3/Relu:0')\n",
        "        self.vgg_fc6_w   = sess.graph.get_tensor_by_name('fc6/weights:0')\n",
        "        self.vgg_fc6_b   = sess.graph.get_tensor_by_name('fc6/biases:0')\n",
        "        self.vgg_fc7_w   = sess.graph.get_tensor_by_name('fc7/weights:0')\n",
        "        self.vgg_fc7_b   = sess.graph.get_tensor_by_name('fc7/biases:0')\n",
        "\n",
        "        layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1',\n",
        "                  'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3',\n",
        "                  'conv5_1', 'conv5_2', 'conv5_3']\n",
        "        for l in layers:\n",
        "            self.l2_loss += sess.graph.get_tensor_by_name(l+'/L2Loss:0')\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_vgg_mods(self):\n",
        "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
        "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
        "                                        name='mod_pool5')\n",
        "        with tf.variable_scope('mod_conv6'):\n",
        "            x = tf.nn.conv2d(self.mod_pool5, self.vgg_fc6_w,\n",
        "                             strides=[1, 1, 1, 1], padding='SAME')\n",
        "            x = tf.nn.bias_add(x, self.vgg_fc6_b)\n",
        "            self.mod_conv6 = tf.nn.relu(x)\n",
        "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc6_w)\n",
        "\n",
        "        with tf.variable_scope('mod_conv7'):\n",
        "            x = tf.nn.conv2d(self.mod_conv6, self.vgg_fc7_w,\n",
        "                             strides=[1, 1, 1, 1], padding='SAME')\n",
        "            x = tf.nn.bias_add(x, self.vgg_fc7_b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv7 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc7_w)\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_vgg_mods_a_trous(self):\n",
        "        sess = self.session\n",
        "\n",
        "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
        "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
        "                                        name='mod_pool5')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Modified conv6\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('mod_conv6'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Decimate the weights\n",
        "            #-------------------------------------------------------------------\n",
        "            orig_w, orig_b = sess.run([self.vgg_fc6_w, self.vgg_fc6_b])\n",
        "            mod_w = np.zeros((3, 3, 512, 1024))\n",
        "            mod_b = np.zeros(1024)\n",
        "\n",
        "            for i in range(1024):\n",
        "                mod_b[i] = orig_b[4*i]\n",
        "                for h in range(3):\n",
        "                    for w in range(3):\n",
        "                        mod_w[h, w, :, i] = orig_w[3*h, 3*w, :, 4*i]\n",
        "            #-------------------------------------------------------------------\n",
        "            # Build the feature map\n",
        "            #-------------------------------------------------------------------\n",
        "            w = array2tensor(mod_w, 'filter')\n",
        "            b = array2tensor(mod_b, 'biases')\n",
        "            x = tf.nn.atrous_conv2d(self.mod_pool5, w, rate=6, padding='SAME')\n",
        "            x = tf.nn.bias_add(x, b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv6 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(w)\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Modified conv7\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('mod_conv7'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Decimate the weights\n",
        "            #-------------------------------------------------------------------\n",
        "            orig_w, orig_b = sess.run([self.vgg_fc7_w, self.vgg_fc7_b])\n",
        "            mod_w = np.zeros((1, 1, 1024, 1024))\n",
        "            mod_b = np.zeros(1024)\n",
        "\n",
        "            for i in range(1024):\n",
        "                mod_b[i] = orig_b[4*i]\n",
        "                for j in range(1024):\n",
        "                    mod_w[:, :, j, i] = orig_w[:, :, 4*j, 4*i]\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Build the feature map\n",
        "            #-------------------------------------------------------------------\n",
        "            w = array2tensor(mod_w, 'filter')\n",
        "            b = array2tensor(mod_b, 'biases')\n",
        "            x = tf.nn.conv2d(self.mod_conv6, w, strides=[1, 1, 1, 1],\n",
        "                             padding='SAME')\n",
        "            x = tf.nn.bias_add(x, b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv7 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(w)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __with_loss(self, x, l2_loss):\n",
        "        self.l2_loss += l2_loss\n",
        "        return x\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_ssd_layers(self):\n",
        "        stride10 = 1\n",
        "        padding10 = 'VALID'\n",
        "        if len(self.preset.maps) >= 7:\n",
        "            stride10 = 2\n",
        "            padding10 = 'SAME'\n",
        "        x, l2  = conv_map(self.mod_conv7,    256, 1, 1, 'conv8_1')\n",
        "        self.ssd_conv8_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv8_1,  512, 3, 2, 'conv8_2')\n",
        "        self.ssd_conv8_2 = self.__with_loss(x, l2)\n",
        "        x, l2  = conv_map(self.ssd_conv8_2,  128, 1, 1, 'conv9_1')\n",
        "        self.ssd_conv9_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv9_1,  256, 3, 2, 'conv9_2')\n",
        "        self.ssd_conv9_2 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv9_2,  128, 1, 1, 'conv10_1')\n",
        "        self.ssd_conv10_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv10_1, 256, 3, stride10, 'conv10_2', padding10)\n",
        "        self.ssd_conv10_2 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv10_2, 128, 1, 1, 'conv11_1')\n",
        "        self.ssd_conv11_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv11_1, 256, 3, 1, 'conv11_2', 'VALID')\n",
        "        self.ssd_conv11_2 = self.__with_loss(x, l2)\n",
        "        if len(self.preset.maps) < 7:\n",
        "            return\n",
        "\n",
        "        x, l2 = conv_map(self.ssd_conv11_2, 128, 1, 1, 'conv12_1')\n",
        "        paddings = [[0, 0], [0, 1], [0, 1], [0, 0]]\n",
        "        x = tf.pad(x, paddings, \"CONSTANT\")\n",
        "        self.ssd_conv12_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv12_1, 256, 3, 1, 'conv12_2', 'VALID')\n",
        "        self.ssd_conv12_2 = self.__with_loss(x, l2)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_norms(self):\n",
        "        x = l2_normalization(self.vgg_conv4_3, 20, 512, 'l2_norm_conv4_3')\n",
        "        self.norm_conv4_3 = x\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __select_feature_maps(self):\n",
        "        self.__maps = [\n",
        "            self.norm_conv4_3,\n",
        "            self.mod_conv7,\n",
        "            self.ssd_conv8_2,\n",
        "            self.ssd_conv9_2,\n",
        "            self.ssd_conv10_2,\n",
        "            self.ssd_conv11_2]\n",
        "\n",
        "        if len(self.preset.maps) == 7:\n",
        "            self.__maps.append(self.ssd_conv12_2)\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_classifiers(self):\n",
        "        with tf.variable_scope('classifiers'):\n",
        "            self.__classifiers = []\n",
        "            for i in range(len(self.__maps)):\n",
        "                fmap = self.__maps[i]\n",
        "                map_size = self.preset.maps[i].size\n",
        "                for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
        "                    name = 'classifier{}_{}'.format(i, j)\n",
        "                    clsfier, l2 = classifier(fmap, self.num_vars, map_size, name)\n",
        "                    self.__classifiers.append(self.__with_loss(clsfier, l2))\n",
        "        with tf.variable_scope('output'):\n",
        "            output      = tf.concat(self.__classifiers, axis=1, name='output')\n",
        "            self.logits = output[:,:,:self.num_classes]\n",
        "        with tf.variable_scope('result'):\n",
        "            self.classifier = tf.nn.softmax(self.logits)\n",
        "            self.locator    = output[:,:,self.num_classes:]\n",
        "            self.result     = tf.concat([self.classifier, self.locator],\n",
        "                                        axis=-1, name='result')\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_optimizer(self, learning_rate=0.001, weight_decay=0.0005,\n",
        "                        momentum=0.9, global_step=None):\n",
        "\n",
        "        self.labels = tf.placeholder(tf.float32, name='labels',\n",
        "                                    shape=[None, None, self.num_vars])\n",
        "        with tf.variable_scope('ground_truth'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Split the ground truth tensor\n",
        "            #-------------------------------------------------------------------\n",
        "            # Classification ground truth tensor\n",
        "            # Shape: (batch_size, num_anchors, num_classes)\n",
        "            gt_cl = self.labels[:,:,:self.num_classes]\n",
        "\n",
        "            # Localization ground truth tensor\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            gt_loc = self.labels[:,:,self.num_classes:]\n",
        "\n",
        "            # Batch size\n",
        "            # Shape: scalar\n",
        "            batch_size = tf.shape(gt_cl)[0]\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute match counters\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('match_counters'):\n",
        "            # Number of anchors per sample\n",
        "            # Shape: (batch_size)\n",
        "            total_num = tf.ones([batch_size], dtype=tf.int64) * \\\n",
        "                        tf.to_int64(self.preset.num_anchors)\n",
        "            # Number of negative (not-matched) anchors per sample, computed\n",
        "            # by counting boxes of the background class in each sample.\n",
        "            # Shape: (batch_size)\n",
        "            negatives_num = tf.count_nonzero(gt_cl[:,:,-1], axis=1)\n",
        "\n",
        "            # Number of positive (matched) anchors per sample\n",
        "            # Shape: (batch_size)\n",
        "            positives_num = total_num-negatives_num\n",
        "\n",
        "            # Number of positives per sample that is division-safe\n",
        "            # Shape: (batch_size)\n",
        "            positives_num_safe = tf.where(tf.equal(positives_num, 0),\n",
        "                                          tf.ones([batch_size])*10e-15,\n",
        "                                          tf.to_float(positives_num))\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute masks\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('match_masks'):\n",
        "            # Boolean tensor determining whether an anchor is a positive\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positives_mask = tf.equal(gt_cl[:,:,-1], 0)\n",
        "\n",
        "            # Boolean tensor determining whether an anchor is a negative\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_mask = tf.logical_not(positives_mask)\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute the confidence loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('confidence_loss'):\n",
        "            # Cross-entropy tensor - all of the values are non-negative\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=gt_cl,\n",
        "                                                            logits=self.logits)\n",
        "            #-------------------------------------------------------------------\n",
        "            # Sum up the loss of all the positive anchors\n",
        "            #-------------------------------------------------------------------\n",
        "            # Positives - the loss of negative anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positives = tf.where(positives_mask, ce, tf.zeros_like(ce))\n",
        "\n",
        "            # Total loss of positive anchors\n",
        "            # Shape: (batch_size)\n",
        "            positives_sum = tf.reduce_sum(positives, axis=-1)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Figure out what the negative anchors with highest confidence loss\n",
        "            # are\n",
        "            #-------------------------------------------------------------------\n",
        "            # Negatives - the loss of positive anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives = tf.where(negatives_mask, ce, tf.zeros_like(ce))\n",
        "\n",
        "            # Top negatives - sorted confience loss with the highest one first\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_top = tf.nn.top_k(negatives, self.preset.num_anchors)[0]\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Fugure out what the number of negatives we want to keep is\n",
        "            #-------------------------------------------------------------------\n",
        "            # Maximum number of negatives to keep per sample - we keep at most\n",
        "            # 3 times as many as we have positive anchors in the sample\n",
        "            # Shape: (batch_size)\n",
        "            negatives_num_max = tf.minimum(negatives_num, 3*positives_num)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Mask out superfluous negatives and compute the sum of the loss\n",
        "            #-------------------------------------------------------------------\n",
        "            # Transposed vector of maximum negatives per sample\n",
        "            # Shape (batch_size, 1)\n",
        "            negatives_num_max_t = tf.expand_dims(negatives_num_max, 1)\n",
        "\n",
        "            # Range tensor: [0, 1, 2, ..., num_anchors-1]\n",
        "            # Shape: (num_anchors)\n",
        "            rng = tf.range(0, self.preset.num_anchors, 1)\n",
        "\n",
        "            # Row range, the same as above, but int64 and a row of a matrix\n",
        "            # Shape: (1, num_anchors)\n",
        "            range_row = tf.to_int64(tf.expand_dims(rng, 0))\n",
        "\n",
        "            # Mask of maximum negatives - first `negative_num_max` elements\n",
        "            # in corresponding row are `True`, the rest is false\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_max_mask = tf.less(range_row, negatives_num_max_t)\n",
        "\n",
        "            # Max negatives - all the positives and superfluous negatives are\n",
        "            # zeroed out.\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_max = tf.where(negatives_max_mask, negatives_top,\n",
        "                                     tf.zeros_like(negatives_top))\n",
        "            # Sum of max negatives for each sample\n",
        "            # Shape: (batch_size)\n",
        "            negatives_max_sum = tf.reduce_sum(negatives_max, axis=-1)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Compute the confidence loss for each element\n",
        "            #-------------------------------------------------------------------\n",
        "            # Total confidence loss for each sample\n",
        "            # Shape: (batch_size)\n",
        "            confidence_loss = tf.add(positives_sum, negatives_max_sum)\n",
        "\n",
        "            # Total confidence loss normalized by the number of positives\n",
        "            # per sample\n",
        "            # Shape: (batch_size)\n",
        "            confidence_loss = tf.where(tf.equal(positives_num, 0),\n",
        "                                       tf.zeros([batch_size]),\n",
        "                                       tf.div(confidence_loss,\n",
        "                                              positives_num_safe))\n",
        "            # Mean confidence loss for the batch\n",
        "            # Shape: scalar\n",
        "            self.confidence_loss = tf.reduce_mean(confidence_loss,\n",
        "                                                  name='confidence_loss')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute the localization loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('localization_loss'):\n",
        "            # Element-wise difference between the predicted localization loss\n",
        "            # and the ground truth\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            loc_diff = tf.subtract(self.locator, gt_loc)\n",
        "\n",
        "            # Smooth L1 loss\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            loc_loss = smooth_l1_loss(loc_diff)\n",
        "\n",
        "            # Sum of localization losses for each anchor\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            loc_loss_sum = tf.reduce_sum(loc_loss, axis=-1)\n",
        "\n",
        "            # Positive locs - the loss of negative anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positive_locs = tf.where(positives_mask, loc_loss_sum,\n",
        "                                     tf.zeros_like(loc_loss_sum))\n",
        "\n",
        "            # Total loss of positive anchors\n",
        "            # Shape: (batch_size)\n",
        "            localization_loss = tf.reduce_sum(positive_locs, axis=-1)\n",
        "\n",
        "            # Total localization loss normalized by the number of positives\n",
        "            # per sample\n",
        "            # Shape: (batch_size)\n",
        "            localization_loss = tf.where(tf.equal(positives_num, 0),\n",
        "                                         tf.zeros([batch_size]),\n",
        "                                         tf.div(localization_loss,\n",
        "                                                positives_num_safe))\n",
        "            # Mean localization loss for the batch\n",
        "            # Shape: scalar\n",
        "            self.localization_loss = tf.reduce_mean(localization_loss,\n",
        "                                                    name='localization_loss')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute total loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('total_loss'):\n",
        "            # Sum of the localization and confidence loss\n",
        "            # Shape: (batch_size)\n",
        "            self.conf_and_loc_loss = tf.add(self.confidence_loss,\n",
        "                                            self.localization_loss,\n",
        "                                            name='sum_losses')\n",
        "            # L2 loss\n",
        "            # Shape: scalar\n",
        "            self.l2_loss = tf.multiply(weight_decay, self.l2_loss,\n",
        "                                       name='l2_loss')\n",
        "            # Final loss\n",
        "            # Shape: scalar\n",
        "            self.loss = tf.add(self.conf_and_loc_loss, self.l2_loss,\n",
        "                               name='loss')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Build the optimizer\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "            optimizer = optimizer.minimize(self.loss, global_step=global_step,\n",
        "                                           name='optimizer')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Store the tensors\n",
        "        #-----------------------------------------------------------------------\n",
        "        self.optimizer = optimizer\n",
        "        self.losses = {\n",
        "            'total': self.loss,\n",
        "            'localization': self.localization_loss,\n",
        "            'confidence': self.confidence_loss,\n",
        "            'l2': self.l2_loss\n",
        "        }\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_names(self):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Names of the original and new scopes\n",
        "        #-----------------------------------------------------------------------\n",
        "        self.original_scopes = [\n",
        "            'conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2',\n",
        "            'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2',\n",
        "            'conv5_3', 'mod_conv6', 'mod_conv7'\n",
        "        ]\n",
        "\n",
        "        self.new_scopes = [\n",
        "            'conv8_1', 'conv8_2', 'conv9_1', 'conv9_2', 'conv10_1', 'conv10_2',\n",
        "            'conv11_1', 'conv11_2'\n",
        "        ]\n",
        "\n",
        "        if len(self.preset.maps) == 7:\n",
        "            self.new_scopes += ['conv12_1', 'conv12_2']\n",
        "\n",
        "        for i in range(len(self.preset.maps)):\n",
        "            for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
        "                self.new_scopes.append('classifiers/classifier{}_{}'.format(i, j))\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_summaries(self, restore):\n",
        "        if restore:\n",
        "            return self.session.graph.get_tensor_by_name('net_summaries/net_summaries:0')\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Build the filter summaries\n",
        "        #-----------------------------------------------------------------------\n",
        "        names = self.original_scopes + self.new_scopes\n",
        "        sess = self.session\n",
        "        with tf.variable_scope('filter_summaries'):\n",
        "            summaries = []\n",
        "            for name in names:\n",
        "                tensor = sess.graph.get_tensor_by_name(name+'/filter:0')\n",
        "                summary = tf.summary.histogram(name, tensor)\n",
        "                summaries.append(summary)\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Scale summary\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('scale_summary'):\n",
        "            tensor = sess.graph.get_tensor_by_name('l2_norm_conv4_3/scale:0')\n",
        "            summary = tf.summary.histogram('l2_norm_conv4_3', tensor)\n",
        "            summaries.append(summary)\n",
        "        return tf.summary.merge(summaries, name='net_summaries')"
      ],
      "metadata": {
        "id": "bJvJEPP9mC_a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ssdutils.py', 'w', encoding='utf-8') as f:\n",
        "  f.write = !wget'https://github.com/sergeyveneckiy/ssd-tensorflow/blob/master/ssdutils.py'\n",
        "with open('ssdvgg16.py', 'w', encoding='utf-8')as f:\n",
        "  f.write = !wget https://github.com/sergeyveneckiy/ssd-tensorflow/blob/master/ssdvgg.py"
      ],
      "metadata": {
        "id": "nw-PM86pqIhy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epuIJ8QaqTGi",
        "outputId": "d2ac8331-c288-40ad-be01-78309c17e807"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'ssdutils.py', 'ssdvgg.py', 'ssdvgg16.py', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "5cybYYTTqeZV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from ssdutils import *\n",
        "from ssdvgg16 import *\n",
        "\n",
        "# Set hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "# Load VGG16 model weights\n",
        "weights = 'vgg16_weights.npz'\n",
        "# vgg16 = load_vgg16(vgg16_weights)\n",
        "vgg16_weights= tf.keras.applications.vgg16.VGG16(weights)\n",
        "\n",
        "# Build SSD model on top of VGG16\n",
        "ssd = build_ssd(vgg16_weights)\n",
        "\n",
        "# Build classifiers for each feature map\n",
        "classifiers = build_classifiers(ssd)\n",
        "\n",
        "# Define loss function for the model\n",
        "loss = ssd_loss\n",
        "\n",
        "# Set up optimizer and learning rate\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
        "                                           decay_steps=10000, decay_rate=0.1)\n",
        "train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "# Load training and validation data\n",
        "(train_images, train_labels), (val_images, val_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_data = list(zip(train_images, train_labels))\n",
        "val_data = list(zip(val_images, val_labels))\n",
        "\n",
        "# Train the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        train_batches = len(train_data) // batch_size\n",
        "        val_batches = len(val_data) // batch_size\n",
        "\n",
        "        # Train on training data\n",
        "        for i in tqdm(range(train_batches)):\n",
        "            batch_data = train_data[i*batch_size:(i+1)*batch_size]\n",
        "            images, labels = preprocess(batch_data)\n",
        "            _, loss_val = sess.run([train_op, loss], feed_dict={ssd.inputs: images,\n",
        "                                                                 ssd.labels: labels})\n",
        "            train_loss += loss_val\n",
        "\n",
        "        # Evaluate on validation data\n",
        "        for i in tqdm(range(val_batches)):\n",
        "            batch_data = val_data[i*batch_size:(i+1)*batch_size]\n",
        "            images, labels = preprocess(batch_data)\n",
        "            loss_val = sess.run(loss, feed_dict={ssd.inputs: images,\n",
        "                                                 ssd.labels: labels})\n",
        "            val_loss += loss_val\n",
        "\n",
        "        # Print epoch results\n",
        "        print('Epoch {}: Train Loss={}, Val Loss={}'.format(epoch+1, train_loss/train_batches,\n",
        "                                                             val_loss/val_batches))\n",
        "\n",
        "    # Save the trained model to a file\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, 'ssd_model.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "UP-MfrAXtAU4",
        "outputId": "ad18e2a7-9bb2-4bf7-a002-af00bbb639ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 8s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_ssd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9b391b975a2c>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Build SSD model on top of VGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mssd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ssd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Build classifiers for each feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_ssd' is not defined"
          ]
        }
      ]
    }
  ]
}